---
title: "Doing Data Science Case Study 2"
author: "Rashmi Patel"
date: "3/26/2021"
output: html_document
---
# Southern Methodist University 6306-Doing Data Science:Case Study 2

In this case study, we will do some analysis on the data given by the client: DDSAnalytics. The client is is interested in knowing the top influential factors that lead to attrition and high monthly income of an employee for their company.

For this analysis, we will do the following steps:

* Import and inspect the data 
* Initial Data Cleaning
* Exploratory Data Analysis of the data
* Building the model
* Predicting the test data using the data
* Storing the predicted data in csv files


# Load the required libraries
```{r setup, echo=FALSE}
knitr::opts_chunk$set(error = TRUE)

library(MASS)
library(car)
library(randomForest)
library(cowplot)
library(corrplot) 
library(leaps)
library(mlbench)
library(dplyr)
library(tidyverse)
library(caret)
library(DataExplorer)
library(gplots)
library(graphics)
library(corrplot)
library(olsrr)
library(ggpubr)
library(rstatix)
library(visdat)
library(GGally)
library(usmap)
library(mice)
library(VIM)
library(plotly)
library(e1071)
library(class)
library(maps)
library(mapproj)
library(stringr)
library(ggthemes)
library(table1)
library(RColorBrewer)

```
# Import and inspect data


## Read the data from GitHub

* The data is fetched from github account.
* The data has 36 variables and 870 entries.
* Plot the histogram for all the variables
```{r}
case2 = read.csv("https://raw.githubusercontent.com/RashmiAPatel19/SMU_MSDS_6306_CaseStudy2_Spring2021/main/CaseStudy2-data.csv", header = TRUE)
head(case2)
dim(case2)
plot_histogram(case2)
str(case2)
```

# Initial Data Cleaning

## Check the variables with vague information and remove the variables

* The variable Over18 has all Y in it as every employee is above 18 years, so we will remove the column.
* The variable EmployeeCount has all 1 in its row, so we will remove the column.
* Since the EmployeeNumber is unique for every employee it will not contribute to the model, so we will remove this column.
* The variable StandardHours, has all values equal to 80, so we will remove it because it will not contribute anything to the model.
* The variable Performance Rating has little vague information in it just 3 and 4, so we will remove it.
* The variables DailyRate, MonthlyRate, and HourlyRate has no meaning as they are all same in sense and related to monthly income.
* After removing these variables we are left with 27 variables and 870 entries.

```{r}
# Checking if the some employees are ages: But no employees were found under 18, so we will remove this variable
unique(case2$Over18)
# Checking for the unique employee count: But every employee count has value=1, so we will remove this variable
unique(case2$EmployeeCount)
#Checking the employee number: But no duplicate values were found, so we will remove this variable
unique(case2$EmployeeNumber)
# Checking he unique values in standard hours: But no values other than 80 found, so we will variable this variable
unique(case2$StandardHours)
# Checking the unique values of Performance rating: But the the values were kind of vague how it is assigned to employees,so we will remove this variable
unique(case2$PerformanceRating)
# Removing the DailyRate, HourlyRate, MonthlyRate because the the meaning is unclear 
case2=select(case2,-c(ID,Over18, EmployeeCount, StandardHours, EmployeeNumber, DailyRate, HourlyRate, MonthlyRate, PerformanceRating))
dim(case2)
colnames(case2)

```

## Plot histogram for analyzing Attrition

* We have plotted the histogram of Attrition to know about the Attrition in this data. We found that 730 employees says No for Attrition in this data and 140 employees says yes to Attrition in this data..
```{r}
ggplot(case2,aes(x=Attrition,fill=Attrition))+geom_bar()+
  geom_text(aes(label=..count..),stat="count",position=position_stack(0.5))


```
## Check the missing values and handling the missing values. 

* We checked for missing values in the data and no missing values were found.
* We converted the character variables into integer.
* We then created a correlation plot to see the correlation between variables
```{r}
# Check for missing values
table(is.na(case2))

# Check for the total number of columns that are character and numeric in type
numeric_var_case2=sum(sapply(case2[,1:27],is.numeric))
numeric_var_case2
char_var_case2=sum(sapply(case2[,1:27],is.character))
char_var_case2
# Check for the names of  columns that are character and numeric in type
numeric_varname_case2=which(sapply(case2[,1:27],is.numeric))
numeric_varname_case2
char_varname_case2=which(sapply(case2[,1:27],is.character))
char_varname_case2

case2_numeric=case2
var_facs <- c("Attrition","EducationField","MaritalStatus","BusinessTravel","JobRole", "Department", "OverTime", "Gender")
case2_numeric[,var_facs] <- lapply(case2[,var_facs] , factor, ordered = FALSE)

case2_numeric$JobRole <- as.integer(case2_numeric$JobRole)
case2_numeric$Department <- as.integer(case2_numeric$Department)
case2_numeric$MaritalStatus <- as.integer(case2_numeric$MaritalStatus)
case2_numeric$BusinessTravel <- as.integer(case2_numeric$BusinessTravel)
case2_numeric$Education <- as.integer(case2_numeric$Education)
case2_numeric$Attrition <- as.integer(case2_numeric$Attrition)
case2_numeric$OverTime <- as.integer(case2_numeric$OverTime)
case2_numeric$Gender <- as.integer(case2_numeric$Gender)
case2_numeric$EducationField <- as.integer(case2_numeric$EducationField)

str(case2_numeric)
```
# Categorize the Monthly Income in "Under 30k, 30k to 60k, 60k to 90k, Over90k" by calculating the Yearly Income 

* We decided to categorize the monthly income in 4 parts and create new column in the data:

** Under 30k for $0-$30000k
** 30k to 60k for $30000k -$60000k
** 60k to 90k for $60000k - $90000k
** Over90k for $90000k and above

* After creating new column we have 28 variables and 870 entries.
```{r}

summary(case2_numeric$MonthlyIncome)

case2_numeric$AnnualIncome=cut(case2_numeric$MonthlyIncome, 
    breaks = c(0,2500,5000,7500,999999), 
    labels = c("Under_30k","30k_to_60k","60k_to_90k","Over_90k") 
    )  

dim(case2_numeric)
```
# Exploratory Data Analysis

## Plot for Annual Income by Total Working Years 

 We decided to categorize the total number of working years in the company:

* Less than 5 Yrs for 0 to <5 years
* Less than 15 Yrs for 0 to <15 years
* Less than 30 Yrs for 0 to <30 years
* Less than 40 Yrs for 0 to <40 years

```{r}

unique(case2$TotalWorkingYears)
max(case2$TotalWorkingYears)
min(case2$TotalWorkingYears)
median(case2$TotalWorkingYears)
table(is.na(case2$TotalWorkingYears))
cut(case2$TotalWorkingYears, 
        breaks = c(-1,5,15,30,100), 
  labels = c("Less than 5 Yrs","Less than 15 Yrs","Less than 30 Yrs","Less than 40 Yrs") 
 ) -> case2$WorkingYears

summary(case2$WorkingYears)
table(is.na(case2$WorkingYears))

ggplot(case2,aes(x=TotalWorkingYears,y=MonthlyIncome,col=TotalWorkingYears))+geom_point()+
  geom_smooth(method="lm")
model1=lm(MonthlyIncome~TotalWorkingYears,data=case2)
summary(model1)
```


## Coorelation plot for numeric variables 

We can see the highest correlation that exists for MonthlyIncome very much align to common sense:

* MonthlyIncome & JobLevel = 0.95
* MonthlyIncome & TotalWorkingYears = 0.78
* MonthlyIncome & YearsAtCompany = 0.49
* MonthlyIncome & YearsInCurrentRole = 0.36

```{r}
corrplot(cor(case2_numeric[,numeric_varname_case2]), order = "alphabet",method="number",
         col = brewer.pal(n = 8, name = "RdBu"))
```
## Plot some of the numeric variables against MonthlyIncome

We observe the following:

* Evidence of a strong relationship with JobLevel
* Evidence of a moderate relationship with TotalWorkingYears
* Evidence of a moderate relationship with YearsAtCompany
* Evidence of a moderate relationship with YearsInCurrentRole

```{r}
ggplot(case2,aes(x=JobLevel,y=MonthlyIncome,col=JobLevel))+geom_point()+
  geom_smooth(method="lm")

ggplot(case2,aes(x=TotalWorkingYears,y=MonthlyIncome,col=TotalWorkingYears))+geom_point()+
  geom_smooth(method="lm")

ggplot(case2,aes(x=YearsAtCompany,y=MonthlyIncome,col=YearsAtCompany))+geom_point()+
  geom_smooth(method="lm")

ggplot(case2,aes(x=YearsInCurrentRole,y=MonthlyIncome,col=YearsInCurrentRole))+geom_point()+
  geom_smooth(method="lm")


```

## Plot for checking normality for Monthly Income and Attrition

We split the income values into separate vectors and run some exploratory analysis on them. We see that there are about 5 times as many employees who stay vs those who leave. It also appears the mean MonthlyIncome for employees who leave is somewhat lower: 

* $4764.79 for employees who leave 
* $6702 for those that stay

There is also greater variance in between the two groups as well:
* The "Yes" group having a standard deviation of $3786.389
* The "No" group having a standard deviation of $4675.472 



```{r}


attrition.No.monthlyIncome=case2%>%dplyr::select(Attrition,MonthlyIncome)%>%filter(Attrition=="No")
dim(attrition.No.monthlyIncome)
attrition.No.monthlyIncome%>%summarise(Mean=mean(attrition.No.monthlyIncome$MonthlyIncome),
                                       Median=median(attrition.No.monthlyIncome$MonthlyIncome),
                                       Standard.Deviation=sd(attrition.No.monthlyIncome$MonthlyIncome),
                                       IQR=IQR(attrition.No.monthlyIncome$MonthlyIncome))

attrition.Yes.monthlyIncome=case2%>%dplyr::select(Attrition,MonthlyIncome)%>%filter(Attrition=="Yes")
dim(attrition.Yes.monthlyIncome)
attrition.Yes.monthlyIncome%>%summarise(Mean=mean(attrition.Yes.monthlyIncome$MonthlyIncome),
                                       Median=median(attrition.Yes.monthlyIncome$MonthlyIncome),
                                      Standard.Deviation=sd(attrition.Yes.monthlyIncome$MonthlyIncome),
                                       IQR=IQR(attrition.Yes.monthlyIncome$MonthlyIncome))


```
## Checking assumptions for Attrition and MonthlyIncome and doing T-test

* Visually looking at the histogram and qqplot, it seems like the Attrition data has some right skewness. 

* Visually looking at the histogram and qqplot, it seems like the Attrition data has some right skewness. 

* Because both distributions differ significantly from normality we log transform them in order to do a t-test and see if there is evidence that the distributions differ significantly from each other. 

* The QQ Plot of the log transformed MonthlyIncome for each group shows much better adherence to normality, and brings the variance of each group much closer together. 

* The T-test itself shows there is a significant difference between the Monthly Incomes of "Yes" vs "No" attrition groups, p-value < 0.0001. This is strong evidence that MonthlyIncome has an effect on Attrition


```{r}
ggplot(attrition.No.monthlyIncome,aes(x=MonthlyIncome))+geom_histogram()
qqnorm(attrition.No.monthlyIncome$MonthlyIncome)
ggplot(attrition.No.monthlyIncome,aes(x=log(MonthlyIncome)))+geom_histogram()
qqnorm(log(attrition.No.monthlyIncome$MonthlyIncome))

ggplot(attrition.Yes.monthlyIncome,aes(x=log(MonthlyIncome)))+geom_histogram()
qqnorm(attrition.Yes.monthlyIncome$MonthlyIncome)
ggplot(attrition.Yes.monthlyIncome,aes(x=MonthlyIncome))+geom_histogram()
qqnorm(log(attrition.Yes.monthlyIncome$MonthlyIncome))

t.test(log(attrition.Yes.monthlyIncome$MonthlyIncome), log(attrition.No.monthlyIncome$MonthlyIncome), var.equal = F) 

```
# Model for MonthlyIncome

## Removing the self-generated variables and Attrition from the dataframe and creating the model for Monthly income
We will create a random forest model for looking at the top influential variables among the 26 variables which leads to higher Monthly Income after removing the self-generated variables and Attrition.

*Salary*

Top 5 most influential were: 

* JobLevel
* TotalWorkingYears
* JobRole
* Age
* YearsAtCompany
```{r}
dim(case2)
colnames(case2)
case2[,-c(2,28,29,30,31)] -> case2_salary
case2_salary_features <- randomForest(MonthlyIncome ~., 
                                     data = case2_salary, 
 importance = TRUE)
colnames(case2_salary)                                    
varImpPlot(case2_salary_features)
importance(case2_salary_features)

```
## Looking at Top 5 infuential variables correlation with  MonthlyIncome

Looking at the correlation plot for the top 5 influential factors that leads to higher monthly income.

* MonthlyIncome & JobLevel = 0.95(strong relationship)
* MonthlyIncome & TotalWorkingYears = 0.78(Moderate relationship)
* MonthlyIncome & JobRole = -0.08(Moderate relationship)
* MonthlyIncome & YearsAtCompany = 0.49(Moderate relationship)
* MonthlyIncome & Age = 0.48(Moderate relationship)

```{r}
income.totalYears=data.frame(TotalWorkingYears=case2$TotalWorkingYears,MonthlyIncome=case2$MonthlyIncome,
                             JobRole=case2_numeric$JobRole,JobLevel=case2$JobLevel,
                             Age=case2$Age,YearsAtCompany=case2$YearsAtCompany)
dim(income.totalYears)
corrplot(cor(income.totalYears), order = "alphabet",
         col = brewer.pal(n = 8, name = "RdBu"),method="number")


```
## Looking at Job Level for Monthly Income

Visually looking at the graph it seems like the employees at Job Level=1 have the earning less than 60k.

Numerically it is approximately 74.26% of the employees at Job Level=1 have the earning less than 60k.

Only 15.67% of the employees earning above 90k are at Job Level=5. No employees at job level=5 are earning below 90k.

```{r}
ggplot(case2,aes(x=JobLevel,fill=AnnualIncome))+geom_bar(position="stack",stat="count")+
  theme(axis.text.x = element_text(angle = 90))+geom_text(aes(label=..count..),stat="count",position=position_stack(0.5))+facet_grid(rows = vars(AnnualIncome))


```
## Plot for Monthly Income by Total Working Years

We have plotted the bar graph to see the annual income of employees based on their total working years in the company.

* We found that only 10.59% of the employees earning above 90k are working in the company for more than 30 years. None employees are earning below 90k  who are working in the company for more than 30 years.
* We also found that none of the employees are earning above 60k who have been working less than 5 years in the company.
```{r}
ggplot(case2,aes(fill=AnnualIncome,x=WorkingYears))+geom_bar(position="stack",stat="count",na.rm = TRUE)+
  facet_grid(rows=vars(AnnualIncome))+
  theme(axis.text.x = element_text(angle = 90))+geom_text(aes(label=..count..),stat="count",position=position_stack(0.5))
```
# Plot for Monthly Income by Job Role 

We have plotted the bar graph to see the annual income of employees based on their job role in the company.

We found that only 27.12% of the 870 employees are earning above 90k.
We also found that every Research Director and Managers in the company have the highest pay which is over 90k.
We see that no sales representative earn over 90k, in which 98.11% earn below 60k and only 1.89% employees earn 60k to 90k.
No healthcare representative anf Manufacturing director and Research Scientist earn below 30k.

```{r}
ggplot(case2,aes(fill=AnnualIncome,x=JobRole))+geom_bar(position="stack",stat="count",na.rm = TRUE)+
  facet_grid(rows=vars(AnnualIncome))+
  theme(axis.text.x = element_text(angle = 90))+geom_text(aes(label=..count..),stat="count",position=position_stack(0.5))
```
# Looking at Years At Company for MonthlyIncome

Visually looking at the graph it seems like the employees working at the company being less than equal to 5 year are more likely to earn below 30k. 

Approximately 27.19% of the employees working at the company for less than 5 years are likely to earn below 30k.

Only 3.38% of the employees earning above 90k are working at the company for more than 30 years.
```{r}
ggplot(case2,aes(fill=AnnualIncome,x=CompanyYears))+geom_bar(position="stack",stat="count",na.rm = TRUE)+
  theme(axis.text.x = element_text(angle=90))+geom_text(aes(label=..count..),stat="count",position=position_stack(0.5))+
facet_grid(rows = vars(AnnualIncome))

```
# Looking at Age for Monthly Income

Only 7.65% of the employees having 19 to 30 years age earn above 90k and approximately 31.53% of the employees earn below 30k.

We found that 57.33% of the employees having age 50 to 60 years are earning above 90k.

```{r}

ggplot(case2,aes(fill=AnnualIncome,x=AgeEmp))+geom_bar(position="stack",stat="count",na.rm = TRUE)+
  theme(axis.text.x = element_text(angle=90))+geom_text(aes(label=..count..),stat="count",position=position_stack(0.5))+
facet_grid(rows = vars(AnnualIncome))


```
## Building a linear regression model for MontHly income vs all variables

* By building the linear regression model we found the following:

** R-squared=0.9491; that means the the model is capable of explaining that 94.91% of the monthly income is associated with all the factors given in the data.

Then looking at the variation inflation factor, we see that the department and job role variables are having VIF higher than 10, so in the next step we will remove these two variables.
```{r}
case2_salary -> case2_salary_reg
# First create model on all variables
lm(MonthlyIncome ~ ., data =  case2_salary_reg) -> case2_salary_lm
summary(case2_salary_lm)
# Then test for VIF
vif(case2_salary_lm)
```
## Removing the department and job role variables and then looking the performace of the model

By building the other linear regression model after removing the variables with high VIF, we found the following:

** R-squared=0.9142; that means the the model is capable of explaining that 91.42% of the monthly income is associated with all the factors given in the data.

Then again looking at the variation inflation factor, we see that the total working years and years at company variables are having VIF higher than 5, so in the next step we will remove these two variables.

```{r}
case2_salary_reg[,-c(3,11)] -> case2_salary_reg.df
# create model on remaining variables
lm(MonthlyIncome ~ ., data =  case2_salary_reg.df) -> case2_salary_lm
summary(case2_salary_lm)
# Then test for VIF
vif(case2_salary_lm)

```
## Removing the total working years and years at company variables and then looking the performace of the model

Even after removing the variables having variables with VIF higher than 5, the model is performing good.

* By building the other linear regression model after removing the variables with high VIF, we found the following:

* R-squared=0.9142; that means the the model is capable of explaining that 91.42% of the monthly income is associated with all the factors given in the data.

```{r}
case2_salary_reg.df[,-c(17,20)] -> case2_salary_reg.df
# create model on remaining variables
lm(MonthlyIncome ~ ., data =  case2_salary_reg.df) -> case2_salary_lm
summary(case2_salary_lm)
# Then test for VIF
vif(case2_salary_lm)

```
## Using StepWise Slection for feature selection

Now that we have used multicollinearity to reduce to 22 parameters, we now will run Stepwise Feature Selection to find the 6 most influential variables and compare them to both what the Random Forest found as well as the prior Linear Regression.

Important Factors that all have p-value <= 0.05:

* BusinessTravel: Travel_Rarely
* DistanceFromHome 
* EducationField: Marketing
* JobLevel
* TotalWorkingYears
* YearsWithCurrManager

We can see the RMSE of this model is $1369.276

And the Adjusted R-squared is 0.9119, which means an estimated 91.19% of the MonthlyIncome variable can be accounted for by this model.

```{r}
dim(case2_salary_reg.df)
trainControl(method = "cv", number = 5) -> train.CV

train(MonthlyIncome ~ .,
  data = case2_salary_reg.df,
  method = "lmStepAIC",
  trControl = train.CV
) -> case2.salary.stepwise

# Final model
summary(case2.salary.stepwise)

# Results including RMSE of final model
case2.salary.stepwise$results

```
## Buliding a Random Forest model and Comparing to Linear Regression Model

We will the variables that were selected by random forest feature selection to build the model.

We found that, the Random Forest performs slightly better with an Adjusted R-squared of 0.9472 and the RMSE of $1069.555 as compared to Linear regression model.


```{r}

train(MonthlyIncome ~ JobLevel + JobRole + TotalWorkingYears + YearsAtCompany + Age,
  data = case2_salary,
  method = "lm",
  trControl = train.CV
) -> case2_salary.stepwise.rf

# Final model
summary(case2_salary.stepwise.rf)

# Results including RMSE of final model
case2_salary.stepwise.rf$results

```
## Predictions for Monthly Income

### Validating the predictions for both Linear regression and Random Forest Model on the training data

We will use the training data and make the predictions using the model and see which model is working better with lower RMSE(prediction error).

* We found that the random forest model are making predictions with lower RMSE(prediction error)=0.1651 as compared to the linear regression model with RMSE(prediction error)=0.2112.


```{r}
# generating predictions on test data
case2$MonthlyIncome_LM <- predict(case2.salary.stepwise, newdata = case2)
case2$MonthlyIncome_LM
case2$MonthlyIncome_RF <- predict(case2_salary.stepwise.rf, newdata = case2)
case2$MonthlyIncome_RF

# Prediction Error for Linear Regression
case2.LM.RMSE=RMSE(case2$MonthlyIncome_LM, case2$MonthlyIncome) / mean(case2$MonthlyIncome)
case2.LM.RMSE
# Prediction Error for Linear Regression
case2.RF.RMSE=RMSE(case2$MonthlyIncome_RF, case2$MonthlyIncome) / mean(case2$MonthlyIncome)
case2.RF.RMSE
```
### Generating the predictions for both Linear regression and Random Forest Model on the original test data for salary

We run the model on the test data sets that do not have the MonthlyIncome parameter

* MonthlyIncome_LM = Predictions for Regression
* MonthlyIncome_RF = Predictions for Random Forest

We have saved the predictions of the original test data in the Salary_LM.csv and Salary_RF.csv

```{r}
NoSalary.df=read.csv("https://raw.githubusercontent.com/RashmiAPatel19/SMU_MSDS_6306_CaseStudy2_Spring2021/main/DDSCaseStudy2CompSet-NoSalary.csv",header=TRUE)
head(NoSalary.df)
dim(NoSalary.df)

NoSalary.df$MonthlyIncome_LM <- predict(case2.salary.stepwise, newdata = NoSalary.df)
head(NoSalary.df$MonthlyIncome_LM)
NoSalary.df$MonthlyIncome_RF <- predict(case2_salary.stepwise.rf, newdata = NoSalary.df)
head(NoSalary.df$MonthlyIncome_RF)
```
## Storing the predictions in the new csv files

We have saved the predictions of the original test data in the Salary_LM.csv and Salary_RF.csv

```{r}
output.LM=data.frame(Id=NoSalary.df$ID,NoSalary.df$MonthlyIncome_LM)
output.RF=data.frame(Id=NoSalary.df$ID,NoSalary.df$MonthlyIncome_RF)

head(output.LM)
head(output.RF)

write.csv(output.LM,file="Salary_LM.csv",row.names = FALSE)
write.csv(output.RF,file="Salary_RF.csv",row.names = FALSE)


```
# Model for Attrition


## Removing the annual income from the dataframe and creating the model for Attrition

We will create a random forest model for looking at the top influential variables among the 27 variables which leads to Attrition after removing the annual income variable

*Attrition*

Top 5 most influential were: 

* OverTime
* MonthlyIncome
* StockOptionLevel
* MaritalStatus
* Age

```{r}

case2_numeric -> case2_attrition
dim(case2_attrition)
# Random forest for Attrition
case2_attrition_features <- randomForest(Attrition ~.-AnnualIncome, 
                                     data = case2_attrition, 
                                     importance = TRUE)
varImpPlot(case2_attrition_features)

```
## Looking at Top 5 infuential variables correlation with  Attrition

* Looking at the correlation plot for the top 5 influential factors that leads to attrition.

** Attrition & OverTime = 0.27
** Attrition &  MOnthlyIncome = -0.15
** Attrition &  StockOptionLevel = -0.15
** Attrition &  Age = -0.15
** Attrition &  MaritalStatus = 0.20


```{r}
attrition.factors=data.frame(OverTime=case2_numeric$OverTime,MonthlyIncome=case2$MonthlyIncome,
                             StockOptionLevel=case2_numeric$StockOptionLevel,Attrition=case2_numeric$Attrition,
                             Age=case2$Age,MaritalStatus=case2_numeric$MaritalStatus)
str(attrition.factors)
corrplot(cor(attrition.factors), order = "alphabet",
         col = brewer.pal(n = 8, name = "RdBu"),method="number")

cor(case2_numeric$MaritalStatus,case2_numeric$Attrition)
```
## Looking at OverTime for Attrition

Visually looking at the graph it seems like the employees doing Over time are more likely to leave.

Numerically approximately 57.14% of the employees doing overTime are likely to leave.
```{r}
ggplot(case2,aes(fill=Attrition,x=OverTime))+geom_bar(position="stack",stat="count",na.rm = TRUE)+
  theme(axis.text.x = element_text(angle = 90))+geom_text(aes(label=..count..),stat="count",position=position_stack(0.5))+facet_grid(rows = vars(Attrition))


```
## Looking at Annual Income for Attrition

Visually looking at the graph it seems like the employees earning Under 30k and between 30k to 60k are more likely to leave.

Numerically it is approximately 70.71% of the employees  with Attrition=Yes.

```{r}
case2$AnnualIncome=case2_numeric$AnnualIncome
dim(case2)
ggplot(case2,aes(fill=Attrition,x=AnnualIncome))+geom_bar(position="stack",stat="count",na.rm = TRUE)+
  theme(axis.text.x = element_text(angle = 90))+geom_text(aes(label=..count..),stat="count",position=position_stack(0.5))+facet_grid(rows = vars(Attrition))


```
## Looking at Stock Option Level for Attrition

Visually looking at the graph it seems like the employees having 0 Stock Option Level are more likely to leave. 

Numerically it is approximately 70.0% of the employees with Attrition=Yes.

```{r}
ggplot(case2,aes(fill=Attrition,x=StockOptionLevel))+geom_bar(position="stack",stat="count",na.rm = TRUE)+
  theme(axis.text.x = element_text(angle = 90))+geom_text(aes(label=..count..),stat="count",position=position_stack(0.5))+facet_grid(rows = vars(Attrition))


```
## Looking at Age for Attrition

Visually looking at the graph it seems like the employees of 19 to 30 years age are more likely to leave. 

Numerically it is approximately 40.71% of the employees with Attrition=Yes.

```{r}
case2$AgeEmp=cut(case2$Age, 
        breaks = c(17,30,40,50,60,100), 
  labels = c("19 to 30 years"," 30 to 40 years","40 to 50 years","50 to 60 years","Above 60 years") 
 ) 

ggplot(case2,aes(fill=Attrition,x=AgeEmp))+geom_bar(position="stack",stat="count",na.rm = TRUE)+
  theme(axis.text.x = element_text(angle = 90))+geom_text(aes(label=..count..),stat="count",position=position_stack(0.5))+facet_grid(rows = vars(Attrition))


```
## Looking at Marital Status for Attrition

Visually looking at the graph it seems like the employees who are single are more likely to leave. 

Numerically it is approximately 50% of the employees with Attrition=Yes.

```{r}
ggplot(case2,aes(fill=Attrition,x=MaritalStatus))+geom_bar(position="stack",stat="count",na.rm = TRUE)+
  theme(axis.text.x = element_text(angle=90))+geom_text(aes(label=..count..),stat="count",position=position_stack(0.5))+
facet_grid(rows = vars(Attrition))

```

## Building the model

Now we will focus on Attrition and will implement the following algorithms for building the model:

### K-Nearest Neighbor(KNN) Classifier
### Random Forest Classifier
### Logistic Regression

First we need to split the data into a training & testing set. 75% of our data will be used for the training set and will be used to create the model. The remaining 20% is for the test set, which will be used to validate actual values vs predicted values using our model. 

We need to measure the models: 

* Accuracy: correct results / all results
* Sensitivity: correctly predicted Attrition:No / All actual "No" 
* Specificity: correctly predicted Attrition:Yes / All actual "Yes" 
```{r}
dim(case2)
case2_attrition.knn=case2[,-c(28,29,30,31,32,33)]
str(case2_attrition.knn)
attrition.partition <- createDataPartition(case2_attrition.knn$Attrition, p = 0.75, list = F)
attrition.train <- case2_attrition.knn[attrition.partition,] 
attrition.test <- case2_attrition.knn[-attrition.partition,]

# validate train and test sets
head(attrition.train)
head(attrition.test)
dim(attrition.train)
dim(attrition.test)

```
## K-Nearest Neighbor for Attrition 

In the K-Nearest Neighbor(KNN), we will use 5-fold cross validation and 50 repeats to create the model for predicting the attrition. 

** We obtained the accuracy above 80%, sensitivity above 80% and specificity above 70% everytime we run the model with different train and test set values.
```{r}
set.seed(200)
train.knn<- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 50,
  summaryFunction = twoClassSummary,
  classProbs = TRUE
)

# KNN on Training Set

attrition.knn <- train(
  Attrition ~ .,
  data = attrition.train,
  method = "knn",
  metric = "Spec",
  trControl = train.knn,
  preProcess = c("center","scale"),
  tuneLength = 20
)

# Adding predictions to Test Data
predict(attrition.knn, newdata = attrition.test ) -> attrition.test$Attrition_KNN
# creating confusion matrix
confusionMatrix(
  table(attrition.test$Attrition, attrition.test$Attrition_KNN )
)

```
## Random Forest for Attrition

In the Random Forest, we will use 5-fold cross validation and 50 repeats to create the model for predicting the attrition. 

** We obtained the accuracy above 85%, sensitivity above 85% and specificity above 70% everytime we run the model with different train and test set values.

```{r}

set.seed(200)

train.knn<- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 50,
  summaryFunction = twoClassSummary,
  classProbs = TRUE
)
train.forest<- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats=25
)
metric="Accuracy"

# Random Forest
train(as.factor(Attrition) ~ .,
  data = attrition.train,
  method = "rf",
  metric = "Accuracy",
  trControl = train.forest
 
  
) -> attrition.rf

predict(attrition.rf, newdata = attrition.test ) -> attrition.test$Attrition_RF
# creating confusion matrix
confusionMatrix(
  table(attrition.test$Attrition, attrition.test$Attrition_RF )
)

```
## Logistic Regression for ATtrition

In the Logistic Regression, we will use 5-fold cross validation and 50 repeats to create the model for predicting the attrition. 

** We obtained the accuracy above 85%, sensitivity above 85% and specificity above 70% everytime we run the model with different train and test set values.

```{r}



train.glm<- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 50,
)
set.seed(200)
attrition.glm.model <- train(as.factor(Attrition)~., attrition.train , method = 'glm',trControl = train.forest)
train=case2_numeric
glm.fit=glm(Attrition~.,data=train)
#Predict
attrition.glm.pred <- predict(attrition.glm.model,newdata=attrition.test)
#Print confusion matrix
confusionMatrix(
  table(attrition.test$Attrition, attrition.glm.pred )
)
```
## Predictions for Attrition

### Validating KNN on Entire Training Data Set

For the prediction on the entire training set, we found the following results:
  
** We obtained the accuracy above 80%, sensitivity above 80% and specificity above 80% everytime we run the model entire training set.

```{r}

# generating predictions on test data
case2$Attrition_KNN <- predict(attrition.knn, newdata = case2)

# creating confusion matrix for KNN
confusionMatrix(
  table(case2$Attrition, case2$Attrition_KNN )
)


```
### Validating Random Forest on Entire Training Data Set

For the prediction on the entire training set, we found the following results:

** We obtained the accuracy above 90%, sensitivity above 90% and specificity above 90% everytime we run the model entire training set.


```{r}

# generating predictions on test data
case2$Attrition_RF <- predict(attrition.rf, newdata = case2)

# creating confusion matrix for RF
confusionMatrix(
  table(case2$Attrition, case2$Attrition_RF )
)



```
### Validating GLM on Entire Training Data Set

For the prediction on the entire training set, we found the following results:

** We obtained the accuracy above 85%, sensitivity above 85% and specificity above 75% everytime we run the model entire training set.



```{r}

# generating predictions on test data
case2$Attrition_GLM <- predict(attrition.glm.model, newdata = case2)

# creating confusion matrix for RF
confusionMatrix(
  table(case2$Attrition, case2$Attrition_GLM )
)



```
## Reading the original test data for Attrition and generating the predictions

The predictions generated by KNN, Random Forest, and Logistic Regression are stored in the original test data setin separate variables with following names :

* Attrition_KNN = Predictions for K-Nearest Neighbor
* Attrition_RF = Predictions for Random Forest
* Attrition_GLM = Predictions for Logistic Regression


```{r}
NoAttrition.df=read.csv("https://raw.githubusercontent.com/RashmiAPatel19/SMU_MSDS_6306_CaseStudy2_Spring2021/main/CaseStudy2CompSet%20No%20Attrition.csv",header=TRUE)
head(NoAttrition.df)
dim(NoAttrition.df)


```
## Storing the predictions in the new csv files

We have saved the predictions of the original test data in the Attrition_KNN.csv, Attrition_RF.csv and Attrition_GLM.csv

```{r}
# creating new attrition in original test data with KNN model predicted Attrition
NoAttrition.df$Attrition_KNN <- predict(attrition.knn, NoAttrition.df)
output.knn=data.frame(Id=NoAttrition.df$ID,NoAttrition.df$Attrition_KNN)

# creating new attrition in original test data with Random Forest model predicted Attrition
NoAttrition.df$Attrition_RF <- predict(attrition.rf, NoAttrition.df)
output.rf=data.frame(Id=NoAttrition.df$ID,NoAttrition.df$Attrition_RF)

# creating new attrition in original test data with Logistic Regression model predicted Attrition
NoAttrition.df$Attrition_GLM <- predict(attrition.glm.model, NoAttrition.df)
output.glm=data.frame(Id=NoAttrition.df$ID,NoAttrition.df$Attrition_GLM)

head(output.knn)
head(output.rf)
head(output.glm)

write.csv(output.knn,file="Attrition_KNN.csv",row.names = FALSE)
write.csv(output.rf,file="Attrition_RF.csv",row.names = FALSE)
write.csv(output.glm,file="Attrition_GLM.csv",row.names = FALSE)


```